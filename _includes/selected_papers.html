<div class="publications">
  <h2>Selected Papers</h2>
  <table>
    <tr>
        <td style="text-align: right;">
            <br>
            <p align="justify"> <img src="../assets/img/research/2021_moment_matching.jpg" style="width:100%;"
                alt="2021_moment_matching" /> </p>
        </td>  
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://gokul.dev/mmil/" target="_blank"><strong>Of Moments and Matching: Trade-offs and Treatments in Imitation Learning</strong></a>
          <br>
          Gokul Swamy, <strong>Sanjiban Choudhury</strong>, Zhiwei Steven Wu, and J Andrew Bagnell
          <br>
          <em>International Conference on Machine Learning (ICML)</em>, 2021
          <br>
          <a href="https://gokul.dev/mmil/" target="_blank">project page</a> / 
          <a href="https://arxiv.org/abs/2103.03236" target="_blank">paper</a> /
          <a href="https://www.youtube.com/playlist?list=PL51kEpt5uSsbZSaGyUMsLsOoFP8-hyx0R" target="_blank">video</a> /
          <a href="https://github.com/gkswamy98/pillbox" target="_blank">code</a> 
          <p></p>
          <p>All of imitation learning can be reduced to a game between a learner (generator) and a value function (discriminator) where the payoff is the performance difference between learner and expert. </p>
        </td>
      </tr>
      <tr>
        <td style="text-align: right;">
            <br>
            <p align="justify"> <img src="../assets/img/research/2019_eil.gif" style="width:100%;"
                alt="2019_eil" /> </p>
        </td>  
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://www.roboticsproceedings.org/rss16/p055.pdf" target="_blank"><strong>Learning from Interventions: Human-robot interaction as both explicit and implicit feedback</strong></a>
          <br>
          Jonathan Spencer, <strong>Sanjiban Choudhury</strong>, Matt Barnes and Siddhartha Srinivasa 
          <br>
          <em>Robotics: Science and Systems (RSS)</em>, 2020
          <br>
          <a href="http://www.roboticsproceedings.org/rss16/p055.pdf" target="_blank">paper</a> /
          <a href="https://www.youtube.com/watch?v=NjkcgB-yy0w" target="_blank">talk</a> 
          <p></p>
          <p> How can we learn from human interventions? Every intervention reveals some information about expert's implicit value function. Infer this function and optimize it. </p>
        </td>
      </tr>
      <tr>
        <td style="text-align: right;">
            <br>
            <p align="justify"> <img src="../assets/img/research/2018_data_driven_planning.png" style="width:100%;"
                alt="2018_data_driven_planning" /> </p>
        </td>  
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/1711.06391" target="_blank"><strong> Data-driven Planning via Imitation Learning </strong></a>
          <br>
          <strong>Sanjiban Choudhury</strong>, Mohak Bhardwaj, Sankalp Arora, Ashish Kapoor, Gireeja Ranade, Sebastian Scherer, Debadeepta Dey
          <br>
          <em>The International Journal of Robotics Research (IJRR)</em>, 2018
          <br>
          <b>
            <font color="red">Finalist for Best Paper of the Year</font>
          </b> <br />
          <a href="https://arxiv.org/abs/1711.06391" target="_blank">paper</a>
          <p></p>
          <p> Train planners (that operate on partial information) to imitate clairvoyant planners (that have full information) to choose optimal planning decisions. (applies to heuristic search, exploration planning, etc)
          </p>
        </td>
      </tr>
</table>
</div>
